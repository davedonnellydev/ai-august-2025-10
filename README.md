# Project 10 #AIAugustAppADay: Quiz Maker

![Last Commit](https://img.shields.io/github/last-commit/davedonnellydev/ai-august-2025-10)

**ğŸ“† Date**: 18/Aug/2025  
**ğŸ¯ Project Objective**: AI generates quizzes based on a given topic.  
**ğŸš€ Features**: User enters a topic, AI generates 3 rounds of 10 questions; user can also adjust level of difficulty overall or per round. Stretch goals: User can choose tone of questions (funny, wacky, intellectual etc); user can export questions and answers?; multiple users can play.  
**ğŸ› ï¸ Tech used**: Next.js, Typescript, OpenAI API  
**â–¶ï¸ Live Demo**: [https://ai-august-2025-10.netlify.app/](https://ai-august-2025-10.netlify.app/)  


## ğŸ—’ï¸ Summary

Todayâ€™s project was a simple quiz maker app. The user could enter a topic, and the app would generate a quiz that could either be exported or taken online. My stretch goal was to allow multiple users to take the same quiz online â€” something Iâ€™ve seen implemented before but never explored in detail. That would have been a great learning opportunity, but it will have to wait for a future version.  

The project itself was fairly straightforward, but what stood out most was the way I used Cursor as a building tool. Iâ€™m noticing that I often accept Cursorâ€™s output at face value: it generates multiple files, I skim them, they look okay, I click accept, and then rely on tests, the linter, formatting, and manual user testing to catch any issues. Only when something fails do I dive into the code more carefully.  

This workflow is efficient in the short term, but it feels risky. As a junior developer, I worry that I might be reinforcing bad habits â€” relying too heavily on AI without truly understanding the details of the code being added to my codebase. At the same time, this challenge is about exploring what AI can do, and part of that is recognising both the benefits and pitfalls of integrating it into daily development.  

**Lessons learned**  
- Stretch goals are great for ambition, but even simple MVPs can highlight important insights.  
- AI can lull you into a false sense of security. Always balance trust with verification.  
- Build in small enough chunks so you know exactly whatâ€™s going into your codebase.  

**Final thoughts**  
This was a good reminder that AI is a tool â€” not a substitute for understanding. While itâ€™s tempting to let it take the wheel, the responsibility for clean, maintainable code still lies with me.  


This project has been built as part of my AI August App-A-Day Challenge. You can read more information on the full project here: [https://github.com/davedonnellydev/ai-august-2025-challenge](https://github.com/davedonnellydev/ai-august-2025-challenge).

## ğŸ§ª Testing

![CI](https://github.com/davedonnellydev/ai-august-2025-10/actions/workflows/npm_test.yml/badge.svg)  
_Note: Test suite runs automatically with each push/merge._

## Quick Start

1. **Clone and install:**

   ```bash
   git clone https://github.com/davedonnellydev/ai-august-2025-10.git
   cd ai-august-2025-10
   npm install
   ```

2. **Set up environment variables:**

   ```bash
   cp .env.example .env.local
   # Edit .env.local with your values
   ```

3. **Start development:**

   ```bash
   npm run dev
   ```

4. **Run tests:**
   ```bash
   npm test
   ```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env.local` file in the root directory:

```bash
# OpenAI API (for AI features)
OPENAI_API_KEY=your_openai_api_key_here

```

### Key Configuration Files

- `next.config.mjs` â€“ Next.js config with bundle analyzer
- `tsconfig.json` â€“ TypeScript config with path aliases (`@/*`)
- `theme.ts` â€“ Mantine theme customization
- `eslint.config.mjs` â€“ ESLint rules (Mantine + TS)
- `jest.config.cjs` â€“ Jest testing config
- `.nvmrc` â€“ Node.js version

### Path Aliases

```ts
import { Component } from '@/components/Component'; // instead of '../../../components/Component'
```

## ğŸ“¦ Available Scripts

### Build and dev scripts

- `npm run dev` â€“ start dev server
- `npm run build` â€“ bundle application for production
- `npm run analyze` â€“ analyze production bundle

### Testing scripts

- `npm run typecheck` â€“ checks TypeScript types
- `npm run lint` â€“ runs ESLint
- `npm run jest` â€“ runs jest tests
- `npm run jest:watch` â€“ starts jest watch
- `npm test` â€“ runs `prettier:check`, `lint`, `typecheck` and `jest`

### Other scripts

- `npm run prettier:check` â€“ checks files with Prettier
- `npm run prettier:write` â€“ formats files with Prettier

## ğŸ“œ License

![GitHub License](https://img.shields.io/github/license/davedonnellydev/ai-august-2025-10)  
This project is licensed under the MIT License.
